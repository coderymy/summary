# JMS和AMQP

都是一种行业规范的API形式。遵循该API能保证消息的基本传递

JMS（Java MessageService）Java的消息服务

AMQP（Advanced Message Queuing Protocal，高级消息队列协议）

**解耦**

1. 消息生产者不去考虑太多七七八八的，只考虑自己去生产这个消息。至于谁需要这个消息，谁获取这个消息失败等
   就不用了消息生产者来考虑。这就对生产者的代码进行了解耦合

   也就是使用发布/订阅模式来进行连接

###### **异步**

也就是，消息中间件可以实现业务逻辑的异步传输

比如，为了达到高可用性。我们项目中将付款和下订单分开来执行。在流量比较大的时候，可以暂存消息中间件。等到消费者空下来，再慢慢执行

虽然这样对用户的那种使用舒适度会有下降。但是还是不能造成任何的数据不一致。所以可以实现

###### **销峰**

可以在业务层和数据库层中间加一个消息中间件。比如说mysql的顶峰处理2万个请求，添加中间件之后，就可以按照mysql自己的意愿，用多少取多少。当然真实场景不是这样的。但是可以这么理解。然后将数据库的访问压力放在别的时候进行平摊

# 1. 四大消息队列

| 特性           | activeMQ                                           | rabbitMQ                                                     | rocketMQ                                                     | kafka                                                        |
| -------------- | -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 单机吞吐量     | 万级-1                                             | 万级别-2                                                     | 10万级-3                                                     | 10万级-4                                                     |
| 消息可靠性     | <font color='red'>有小概率<br/>会丢消息</font>     |                                                              | 通过配置0丢失                                                | 通过配置0丢失                                                |
| 优点           | 很成熟 <br>业内使用很多                            | 性能较高<br>自带管理后台<br>社区活跃                         | 性能高<br>默认支持分布式架构<br>Java开发好维护<br><font color="red">支持复杂业务场景</font> | 性能高<br>                                                   |
| 缺点           | 有小概率会丢失消息<br>社区维护也较少<br>吞吐量较小 | 性能不够高<br>不默认支持一些复杂的业务操作，需要自己实现     |                                                              | 依赖于Zookeeper                                              |
| 特点           |                                                    |                                                              | 顺序消费、pull和push、分布式                                 | 零拷贝。partition                                            |
| 高可用解决方案 | 不支持                                             | 三种模式：<br>单机<br>普通集群：相当于主备机器，但是业务数据不备份。<br>镜像集群：所有MQ上都有全套的业务数据（通信同步压力大） | 基于broker的分布式实现管理。对broker进行多主多从配置         | 与rocketmq类似：通过broker进程进行同步调度,会将消息分摊到不同的partition上 |
|                |                                                    |                                                              |                                                              |                                                              |

业务上推荐使用RocketMq：自身是支持分布式的架构可实现三高。针对各种复杂业务提供了解决方案（延时队列、顺序消费、集合数据等）

中间件上（日志采集，数据分析等）推荐使用kafka：稳定高效以及可靠的持久性特点。



# 2. 工作原理

## 2.1 消息生产过程

1. Producer发送消息之前，去NameServer获取Topic路由信息的请求。返回topic的`路由表`和`broker列表`

   > 路由表：Map<TopicString,List<QueueData>>，其中QueueData是一个Broker中所有该topic的Queue的集合
   >
   > Broker列表：就是所有Broker组实例（一个Master和一个Slave）组成的Map集合

2. 根据Queue的选择算法，选择一个Queue用于后续消息存储

   > 无序消息
   >
   > 轮询算法
   >
   > 最小投递延迟算法（统计每次投递的投递时间，选择延迟最小的那个Queue。）

3. 进行一些消息的基本处理（压缩4M以下，校验等）

4. 向选出的Queue的Broker发送RPC请求发送消息



## 2.2 消息存储

存储在`/store`目录下

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/0uwJK8.jpg)



### commitlog

存放着mappedFile文件。文件名为当前文件第一条的offset。文件大小为1G上限

也就是该文件中存储的就是整个消息的各个信息。所有topic的消息都顺序放在该文件中



### consumequeue

/consumequeue/${topicName}/${queueId}

用来记录每个Queue的消息索引

### index

其中写入了消息中包含`key`的消息。提供给其他操作需要使用key进行查询的时候使用，快速索引查找



### 零拷贝机制

[大佬零拷贝机制](https://www.likecs.com/show-204169190.html)

正常进行文件传输的时候，需要两个步骤

1. 从用户态切换到内核态：去将磁盘的文件拷贝到内核态缓冲区
2. 内核态切换到用户态：将文件从内核态缓冲区拷贝到用户缓冲区
3. 用户态切换到内核态：用户缓冲区拷贝到socket缓冲区
4. 内核态切换到用户态：发送文件

经历了三次拷贝，四次状态切换

操作磁盘文件需要用户态和内核态之间的切换

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/3FnGSF.png)



**零拷贝是指CPU不需要在应用内存和内核内存之间拷贝数据消耗资源。**

#### mmap

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/sPkl34.png)

原理就是映射一块内存分配区域，给用作共享缓冲区。

增加内核态和用户态都能操作的“共享缓冲区”

1. 磁盘到共享缓冲区
2. 共享缓冲区到socket缓冲区

减少了一次拷贝



RocketMq中关于零拷贝的实现就给予mmap。将文件映射一个内存地址以供调用，真实的拷贝发生在共享缓冲区之间传输的操作上

#### sendfile

原理就是不经过用户态直接在内核态上进行数据拷贝。

## 2.3 pull和push

pull就是客户端去broker获取消息。不断的请求，请求不到就结束过一段时间再请求。

**push**就是主动推送给消费者消息，本质是对pull的包装，建立长链接（长轮询），服务端一旦有数据就会将数据发送给客户端。

长轮询的方式：客户端请求broker获取消息，如果broker没有消息就挂起这个链接并不返回，直到客户端超时或者有消息才返回。

优缺点：

> push

最常用的模式

优点在于，数据的同步及时。

缺点在于服务端不知道客户端的承受能力是多少容易出现数据的堆积在客户端

> pull

优点在于对客户端的压力不大，客户端能承受多少就拉取多少

缺点在于无法评估拉取的时间间隔。



## 2.4 消费模式

**广播模式**

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/mv3P6V.jpg)

消息每个消费者都会消费一份



**集群模式**

基于消费者组的概念，每个消费者组中只有一个消费者会消费消息

![输入图片说明](https://bright-boy.gitee.io/technical-notes/rocketmq/images/QQ%E6%88%AA%E5%9B%BE20220208141053.png)







## 2.5 Rebalance

重新平衡机制：目的就是在于新增消费者之后，将该topic中的所有queue进行平衡分配一下。

问题：在于重新分配的间隙可能会产生**重新消费**、**消费暂停**的问题



## 2.6 queue的分配策略

+ 平均分配

+ 环型平均分配，这样就可以将queue依次分配，不用计算每个consumer分配几个queue的问题

+ 一致性hash算法：最常见的形式了，将queue进行取模运算。适用于频繁变化consumer数量的情况

+ 同机房策略

  ![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/tXm2ul.jpg)





# 3. 深入理解rocketmq

## 3.1 Queue（队列）

消息存储的物理结构，也就是**分区**（kafka中的partition）。

存在的目的：就是为了区分出来消费者。一个topic一般有多个queue。一个消费者针对同一个TOPIC只能消费一个Queue。（不考虑消费者组的问题）

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/dAYp23.png)

> ps：如果消息堆积，增加消费者能解决问题吗？
>
> 如果MessageQueue的数量大于消费者的数量，增加消费者肯定是可以解决问题的！

## 3.2 broker的集群

NameServer每隔10s检测一次Broker的心跳时间，120s没有发送的Broker就认定为宕机；broker每隔30s向NameServer发送一个心跳包并携带时间戳

生产者在发送消息之前先从NameServer获取Broker服务器的地址列表，然后 根据负载算法从列表中选择一台broker发送消息。生产者每**30s**向NameServer更新Topic

### 数据复制与刷盘策略

**数据复制**

master与slave之间的关系

+ 同步复制：master接收到生产者消息，只有通知到slave并返回成功master才返回生产者成功
+ 异步复制：master接收到生产者消息之后直接ACK。之后通过异步的方式通知slave（降低系统写入延迟，效率高，但是出现异常可能会丢失消息）

**刷盘策略**

主机内部内存与磁盘之间的关系

消息发送到broker内存后消息持久化到磁盘的方式

- 同步刷盘：当消息持久化到broker的磁盘后才算是消息写入成功。
- 异步刷盘：当消息写入到broker的内存后即表示消息写入成功，无需等待消息持久化到磁盘。（降低系统写入延迟，效率高，但是出现异常可能会丢失消息）

### 多master

多个master构成，没有slave。**同一个Topic的Queue平均分配在各个master上**

+ 优点：配置简单。磁盘配置为RAID10（保障内存能写入磁盘），同步刷盘（保障消息不会从生产的角度上丢失）消息不会丢失
+ 缺点：宕机期间该server上的消息无法恢复

### 多master多slave

**异步复制**

在配置了RAID磁盘阵列的情况下，一个master一般配置一个slave即可。master与slave的关系是主备关系，即master读写请求，而slave消息的备份与master宕机后的角色切换。

异步复制即前面所讲的`复制策略`中的`异步复制策略`

该模式的最大特点之一是，**当master宕机后slave能够`自动切换`为master**。不过当master宕机后，这种异步复制方式可能会存在少量消息的丢失问题。

>  对于Master的RAID磁盘阵列，若使用的也是异步复制策略，同样也存在延迟问题，同样也可能会丢失消息。但RAID阵列的秘诀是微秒级的（因为是由硬盘支持的），所以其丢失的数据量会更少。

**同步复制**

该模式是`多Master多Slave模式`的`同步复制`实现。该模式与`异步复制模式相比`，优点是消息的安全性更高，不存在消息丢失的情况。但性能要略低。

该模式存在一个大的问题：对于目前的版本，Master宕机后，Slave`不会自动切换`到Master。



总结：

多master的目的是：负载broker上topic的请求（单点变多点同时分布式支持高并发）。

带上slave的目的是：防止master怠机导致master上的消息延时，同时保障消息可靠性。还做到了读写分离防止压力太大。

使用RAID10阵列的目的是：降低刷盘效率问题，保障异步刷盘模式下消息丢失的问题



### 最佳实践

一般会为Master配置RAID10磁盘阵列，然后再为其配置一个Slave。即利用了RAID10磁盘阵列的高效、安全性，又解决了可能会影响订阅的问题。

> 1 ）RAID磁盘阵列的效率要高于Master-Slave集群。因为RAID是硬件支持的。也正因为如此，所以RAID阵列的搭建成本较高。
>
> 2 ）多Master+RAID阵列，与多Master多Slave集群的区别是什么？
> 1.多Master+RAID阵列，其仅仅可以保证数据不丢失，即不影响消息写入，但其可能会影响到消息的订阅。但其执行效率要远高于`多Master多Slave集群`
> 2.多Master多Slave集群，其不仅可以保证数据不丢失，也不会影响消息写入。其运行效率要低于`多Master+RAID阵列`





## 3.3 点对点和发布订阅

## 3.4 rocketmq特点

NameServer：类似于分布式里面的consul、eureka、zookeeper这种注册中心。

- `生产者`发送消息到`NameServer`
- `NameServer`将该消息持久化存储
- `消费者`订阅需要的主题消息
- `NameServer`将消息推送push到`消费者`（也可以`消费者`主动去`NameServer`pull）

broker与nameserver的心跳机制：

```
在启动broker的时候，向nameserver进行注册。
nameserver与每台broker保持长连接，并间隔30s检测broker是否存活，如果检测不到，即将其在路由注册表中移除。
```

多台NameServer之间

```
为了实现高可用，通常会建立多台NameServer，但是相互之间不进行通信。
```

producer、NameServer、broker的路由机制？？

```
每次生产者生产消息时就去NameServer获取服务器地址列表，再通过负载算法选择一台Broker发送消息。
```











# 4. RocketMQ面对几大问题

## 基于磁盘操作的RocketMq性能问题

**零拷贝机制**



**PageCache机制**：页缓存机制



RocketMQ中，无论是消息本身还是消息索引，都是存储在磁盘上的。其不会影响消息的消费吗？当然不会。其实RocketMQ的性能在目前的MQ产品中性能是非常高的。因为系统通过一系列相关机制大大提升了性能。

首先，RocketMQ对文件的读写操作是通过`mmap零拷贝`进行的，将对文件的操作转化为直接对内存地址进行操作，从而极大地提高了文件的读写效率。

其次，consumequeue中的数据是顺序存放的，还引入了`PageCache的预读取机制`，使得对consumequeue文件的读取几乎接近于内存读取，即使在有消息堆积情况下也不会影响性能。

> PageCache机制，页缓存机制，是OS对文件的缓存机制，用于加速对文件的读写操作。一般来说，程序对文件进行顺序读写的速度几乎接近于内存读写速度，主要原因是由于OS使用PageCache机制对读写访问操作进行性能优化，将一部分的内存用作PageCache。
>
> 1)写操作：OS会先将数据写入到PageCache中，随后会以异步方式由pdæush（page dirty æush)内核线程将Cache中的数据刷盘到物理磁盘
> 2)读操作：若用户要读取数据，其首先会从PageCache中读取，若没有命中，则OS在从物理磁盘上加载该数据到PageCache的同时，也会顺序 对其相邻数据块中的数据进行预读取。

RocketMQ中可能会影响性能的是对commitlog文件的读取。因为对commitlog文件来说，读取消息时会产生大量的随机访问，而随机访问会严重影响性能。不过，如果选择合适的系统IO调度算法，比如设置调度算法为Deadline（采用SSD固态硬盘的话），随机读的性能也会有所提升。

## 重复消费

> 原因

+ 消息重发
+ 分布式消息重复股消费

```sequence
title: kafka消息重复消费
participant 生产者 as producer
participant kafka
participant zookeeper as zk
participant 消费者 as consumer


producer -> kafka: 1. 发送每台消息都会带着一个offset(坐标,顺序)

kafka-> consumer: 2. 消费者消费消息是按照顺序消费
consumer-> zk: 3. 消费者消费了消息之后会返回提交offset
zk-> kafka: 4. 同步

```

消费者并不是消费一条数据就提交offset,而是定时定期去提交

如果在已消费、未提交前出现了问题,就会出现kafka认为这条数据并没有消费的情况,下一次消费的时候kafka会将这个offset对应的消息再发给消费者



> 解决方案

幂等:

业务逻辑进行处理

redis锁进行处理

关键业务数据的mysql写入唯一性约束



## 顺序消费

> 原因

rabbitMQ,一个队列,多个消费者,消费消息可能会因为性能不一致等原因导致顺序与生产顺序不一致

kafka,能内部保障写入一个partition的数据是顺序的(在写入的时候指定一个key,这样可以保障所有同样key的数据被同一个partition消费)。kafka不顺序消费的情况是消费者内部的多线程处理

rocketmq，能保证同一个Queue是顺序消费的。所以实现顺序消费的基础就是将消息都发送到该topic的一个Queue中。

> 解决方案

使用针对订单的订单号hash的方式进行运算,保证同一条订单的信息走到同一个MessageQueue中

kafka

+ 使用相同的key,保证进入同一个partition中
+ 出队之后放入内存队列中,保证写入内存队列顺序写入,这样就会进入同一个线程处理

针对重要的数据,也可以增加redis锁机制来保证一定的消费顺序消费的情况



如果这种顺序消息消费失败了,不能使用`CONSUMER_LATER`来将消费发送会broker进行重试消费,而是将整个queue中的信息都回馈回去进行重试.即**不能针对消息重试,而需要针对queue进行重试**

**终极：针对所有mq都适用的解决方案:针对不同的业务信息,创建多个队列.顺序的队列进行使用消息,也就是一个消费者是另一个数据的生产者**





## 消息丢失

1. 生产者弄丢数据<br>
   ribbitMQ使用transaction和confirm模式来保证生产者不会丢消息<br>

   tacnsaction机制：发送消息前"channel.txSelect()"开启事务。消息发送没出现异常"channel.txCommit()"提交事务。出现任何异常"channel.txRollback()"回滚事务<br>
   缺点：吞吐量下降

   confirm机制：所有被生产者生产的消息都会被**加上唯一ID**，如果消息被正确消费了ribbitMQ就发送一个ACK给生产者。如果消息没有正确消费ribbitMQ就返还一个NACK给生产者

2. 消息队列丢失数据
   使用消息持久化磁盘的方法<br>

   1. 开启持久化磁盘的方法
      1. durable设置成true
      2. 发送消息的时候将deliveryMode=2
   2. 设置之后，和confirm一起使用，会在本地持久化存储生产者生产的消息。并在意外岱机之后重启恢复数据

3. 消费者丢数据
   一般是采用了自动确认消息模式。这种模式下，消费者会自动确认收到的消息。然后ribbitMQ会删除这个消息。如果这个时候消费者出现异常没有完整处理业务逻辑，就会出现数据丢失<br>
   解决方法：手动确认消息即可
   <br>
   关闭rabbitMQ提供的自动ACK消息确认机制，改为消费者处理完消息之后,
    手动ACK(基于tcp的ack事务包)

> ###### rabbitmq消息丢失
>
> 1. 创建queue指定durable,也就是持久化的
> 2. 生产消息的时候也指定持久化
>
> 在还没有将消息送入mq的时候,如果宕机会丢失

## 高可用

## 延时队列

实现原理：

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/IDTNNu.jpg)





在broker启动的时候会启动对应数量（对应延时等级的个数）的定时器TImer。每个定时器绑定一个延迟等级，并按照延迟等级每隔一段时间检测一次消息是否到期。

1. 消息发送到对应Topic的queue中，写入commitlog后会检查是否携带了延时队列的标示。
2. 将对应延时消息发送到该Topic的`SCHEDULE_TOPIC_TOPICNAME`中。
3. 定时器会每次检索该延迟等级的第一个消息是否到期，如果到期就执行该消息将该消息重新分发commitlog的该正常队列中
4. 就可以正常执行该消息的消费。



重点在于：延时是在进行消费分发时候做的





## 与kafka的对比

相同点：

+ 都采用了分区的概念，rocketmq中是queue；kafka中是partition
+ 都采用了offset的概念进行消息顺序操作。

不同点：

+ kafka中没有tag的概念
+ kafka中无索引文件



## 消息清理

commitlog默认过期时间为三天，按照文件来清理

## 消息堆积

两种情况：

如果queue本身便比consumer多。那么多上线几个consumer可以缓解消息堆积的问题

如果queue本身不多于consumer，那么上线多个consumer是没有用的。此时应该上线代码将该topic中的数据转移到另一个topic中，然后开多个queue与consumer进行消费处理



## 分布式事务

Rocketmq实现分布式事务。

通过向broker发送half消息。只有在TM获取到所有都准备好的指令之后，才会向broker发送commit/rollback消息，这个时候这个消息才能被消费者看到，才来让其他的所有RM都执行对应的逻辑进行提交



## 批量消息

批量发送的消息不能是延时消息或者事务消息。默认消息体最大上线为4M，可配置（需配置生产者和消费者两端）



# 4. RAID

待续。。。



# 0. 研习《RocketMQ技术内幕》

## 1. rocketMQ的设计目标

+ 架构模式

  发布订阅的模式

+ 顺序消息

  保证发送在同一个TOPIC中的消息在消费的时候是按照发送的顺序进行消费的

  在分布式的情况下如何保证？

+ 消息过滤

  在同一个TOPIC下设置了TAG和`selectorExpression`等元素来帮助消费者进行消息的过滤获取想要的消息

+ 消息存储

  所有TOPIC消息都存储在一个文件中，同时设置过期时间和存储空间报警机制

+ 高可用性

  分布式的架构、消息同步/异步刷盘模式

+ 消费的低延时

  使用长轮询机制（push）

+ 消息回溯

  使用offset的概念帮助进行消息的回溯

+ 消息堆积

  磁盘存储，所以面对消息堆积压力不是很大

+ 定时消息

  为了减少服务端的压力，只允许使用固定模式的时间进行设置定时消息

+ 消息重试

  使用offset

## 2. 路由中心——NameServer

目的：为了保证客户端出现故障能够快速进行切换使用，从而避免服务宕机和消息丢失

，

生产者与broker、NameServer：生产者在发送消息之前先从NameServer获取Broker服务器的地址列表，然后 根据负载算法从列表中选择一台broker发送消息。

NameServer之间：不会相互通信，所以可能会出现短时间的数据不一致性

broker与NameServer：Broker在启动时向所有NameServer注册。	Broker 启动之后会向所有 NameServer 定期（每 **30s**）发送心跳包，包括：IP、Port、TopicInfo，NameServer 会定期扫描 Broker 存活列表，每**10s**检测一次心跳包如果超过 **120s** 没有心跳则移除此 Broker 相关信息，代表下线。

生产者与NameServer：每**30s**向NameServer更新Topic





## 3. 集群、主从

brokerId=0表示主节点

brokerId>0表示从节点

## 4. 消息发送

+ 同步
+ 异步
+ 单向

消息发送者首次向topic发送消息会去nameServer验证是否有这个topic。有就将这个topic缓存到本地缓存中。然后每隔30s去nameServer验证下本地缓存的topic状态





​	





























