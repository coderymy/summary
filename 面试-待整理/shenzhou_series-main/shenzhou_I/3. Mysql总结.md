# 0.学习

[MySQL优化/面试，看这一篇就够了](https://www.nowcoder.com/discuss/150059?type=0&order=0&pos=13&page=0)



# 1. Mysql的基本操作

## 1. 表结构操作

```
1. 创建表
	create table <表名> (
		<字段名> <类型> [列级别约束] [默认值] [注释],
		<字段名> <类型> [列级别约束] [默认值] [注释],
		CONSTRAINT <约束名> FOREIGN KEY (A表的X字段名) REFERENCES B表(Y字段),
		[表级别约束]
	)[engine=引擎][default charset=字符集]
	CREATE TABLE `testdemo1`(
	`uid` INT PRIMARY KEY AUTO_INCREMENT,
	`username` VARCHAR(20) NOT NULL ,
	`password` VARCHAR(20) NOT NULL
	);
	
2. 主键约束
	1. Primary key
	2. 如果是联合主键，可以在表的创建结尾部分使用以下方式创建
		PRIMARY KEY(字段一，字段二)
3. 外键约束（reference）
	1. 在创建列的时候使用以下方式加上外键约束
		字段名 类型 REFERENCES B表(X字段)
	2. 在表创建尾部使用
		CONSTRAINT <约束名> FOREIGN KEY (A表的X字段名) REFERENCES B表(Y字段)
4. 复制表操作
	CREATE TABLE testdemo2 AS SELECT * FROM testdemo1;
5. 修改表
	alter table XXX XXXXXXXXXXX
6. 创建删除索引
	create index 索引名 on 表名(字段名)
	drop index 索引名;
```

> 约束条件

1. 主键约束
   PRIMARY KEY
2. 自动递增	
   auto_increment
3. 外键约束	
   REFERENCE，reference

## 2. 增删改查

### 2.1 增

```
insert into 表名(字段名1，字段名2，字段名3) values(值1，值2，值3);
```

### 2.2 删

```
delete from 表名 where 条件
```

### 2.3 改

```
update table set 列名1=值1，列名2=值2，where 条件
```

### 2.4 查

> #### 单表查询
>
> 1. 排序
>
> ```
> select * from 表名 order by 列名 DESC/ASC
> ```
>
> 2. IN
>
> ```
> select * from 表名 列名 in (值1，值2，值...)
> ```
>
> 3. 模糊查询 
>
> ```
> select * from 表名 列名 like 通配符
> 
> ```
>
> %. 表示匹配出现的任意次数的任意字符
>
> _. 匹配一个字符
>
> 4. 聚合函数，也就是在查询的信息中使用
>
> > 1. AUG()，平均值
> >
> > ```
> > select AUG(uid) from 表名
> > 
> > ```
> >
> > 2. COUNT()，某列数量
> > 3. MAX
> > 4. ()，某列最大值
> > 5. MIN()，某列最小值
> > 6. SUM()，某列的和
>
> 5. 分组查询
>
> > GROUP BY，表示分成什么组
>
> 6. HAVING子句
>
> 用于指定组所对应的条件，也就是可以是聚合函数。这就是和where的唯一区别，都是作为一种条件判断使用的
>
> 7. LIMIT子句
>
> 表示分页查询，指定

> #### 关联表查询
>
> 1. 基本使用
>
> ```
> select * from 表1,表2 where 主表.主键=从表.外键 and 其它过滤条件 
> ```
>
> 2. 内连接
>
> ```
> SELECT 主表.列1, ..., 从表.列1, ... FROM 主表 INNER JOIN 从表 ON 主表.主键 = 从表.外键 WHERE 过滤条件;
> 也就是在以前的基础上，在两个表中间加上了"inner join" 后面的链接条件加上"on"
> ```
>
> 3. 左外连接
>
> ```
> SELECT * FROM ... 
> LEFT JOIN ... ON ... = ...
> WHERE ...;
> 
> 和自连接的区别在于使用"left join"
> ```
>
> 4. 右外连接
>
> ```
> SELECT * FROM ... 
> RIGHT JOIN ... ON ... = ...
> WHERE ...;
> 
> 区别在于使用 "right join"
> 
> ```
>
> 5. 自连接
>
> 关键是使用别名

# 2. 分库分表

## 大表优化

当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下：

### 1. 限定数据的范围

务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；

### 2. 读/写分离

经典的数据库拆分方案，主库负责写，从库负责读；

### 3. 垂直分区

 **根据数据库里面数据表的相关性进行拆分。** 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。

 **简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。** 如下图所示，这样来说大家应该就更容易理解了。
 ![数据库垂直分区](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/数据库垂直分区.png)

- **垂直拆分的优点：** 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。
- **垂直拆分的缺点：** 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；

### 4. 水平分区

**保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。** 

 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。

![数据库水平拆分](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/数据库水平拆分.png)

水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 **水平拆分最好分库** 。

水平拆分能够 **支持非常大的数据量存储，应用端改造也少**，但 **分片事务难以解决**  ，跨节点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 **尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度** ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。

**下面补充一下数据库分片的两种常见方案：**

- **客户端代理：**  **分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。** 当当网的 **Sharding-JDBC** 、阿里的TDDL是两种比较常用的实现。
- **中间件代理：** **在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。** 我们现在谈的 **Mycat** 、360的Atlas、网易的DDB等等都是这种架构的实现。

详细内容可以参考： MySQL大表优化方案: [https://segmentfault.com/a/1190000006158186](https://segmentfault.com/a/1190000006158186)

> 问题

+ 什么叫做垂直拆分,什么叫做水平拆分

  水平拆分,表结构是一致的,业务数据不一样而已

  垂直拆分,表结构不一致.业务数据多张表统一管理一个用户

+ 分库分表中间件的使用

+ 如何做到线上动态的从未分库分表切换成分库分表的形式

+ 如何解决分页问题

+ 主键id不能使用自增主键(雪花算法、全局主键等),sharding-jdbc自带解决方案

主从复制的主从同步延时问题

使用`show status`查看Seconds_Behind_Master结果,即从库延时的时间



## Sharding-JDBC

1. 配置jar文件

2. 配置**数据源**

3. 配置**表数据的分布情况**,即t_order操作哪些表(例:spring.shardingsphere.sharding.tables.t_order.actual-data-nodes=m1.t_order_$->{1..2}标识使用m1这个数据源的表开头为t_order_之后的其他数据)

4. 配置**主键生成策略**SNOWFLAKE

5. 配置表**分片策略及分片算法**.即后续如何找到对应的表的数据

   ![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/hrtlss.jpg)









# 3. Mysql锁机制

[锁机制面试总结](https://zhuanlan.zhihu.com/p/410679024)

# 4. Mysql事务

[事务总结](https://zhuanlan.zhihu.com/p/415377781)

## 4.1 事务的ACID特性

## 4.2 在并发情况下ACID带来的问题

## 4.3 事务的隔离级别来解决问题

## 4.4 MVVM的实现来优化隔离级别带来的问题





# 5. Mysql索引

[索引](https://zhuanlan.zhihu.com/p/412781061)

# 6. Mysql左链接、右链接

+ 左连接

  返回左表的所有行,右边匹配左边对应的连接数据,如果没有则为null

+ 右链接

  与左连接相反

+ 内连接

  只返回两张表都有的同一个id的数据

+ 笛卡尔传递

  返回两张表数据的交集

# 7. redolog、undolog和binlog

## redolog(保证事物提交的持久性)

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/N8wj7n.jpg)
默认有四个文件进行循环存储.

也就是在进行数据库操作的时候,并不是实时就将数据更新到磁盘中,而是一个异步的流程.如果进行update操作,分为以下几个步骤

WAL机制(write ahead logging)

1. 数据库进行sql校验是否能够执行
2. 进行资源锁定(事务不一定存在)
3. write pos写入redolog并于client交互返回成功
4. checkpoint顺序执行执行到这条语句进行磁盘的数据update操作
5. 并擦除这条redolog操作行为




## undolog(保证事物的回滚和MVCC中的版本概念)


记录了一条增删改操作的相反数据变更,也就是变更前的数据记录信息.方便在回滚时候进行数据变更回去

也记录MVCC中的一个版本号.当用户读取一行记录时,若该记录已经被其他事务占用,当前事务可以通过undo读取之前的行版本信息,以此实现非锁定读取。


## binlog

记录实际最终对数据的操作的语句,记录的是二进制信息


## redolog和binlog

1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。
2. binlog是追加写,redolog是循环写

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/m17UMw.jpg)



# 8. 主从复制

默认使用<font color="red">异步</font>的方式进行主从复制，这样从节点不需要一直访问主服务器进行更新

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/1. 主从复制原理.PNG)

涉及三个线程

1. 主服务器的**binary log dump 线程**
2. 从服务器的**I/O线程**
3. 从服务器的**SQL线程**

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/2. 主从复制原理 .PNG)

<font color="red">复制过程：</font>

1. 从节点上的IO进程连接主节点，并请求获取从指定的日志的指定位置之后的日志内容
2. 主节点获取请求后，将对应的日志内容返回给从节点，还包括本次返回的信息的bin-log file 的以及bin-log position
3. 从节点接收数据后，将接收到的日志内容更新到本机的relay log中，并将读取到的binary log文件名和位置保存到master-info 文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log 的哪个位置开始往后的日志内容，
4. Slave 的 SQL线程检测到relay-log 中新增加了内容后，会将relay-log的内容解析成在祝节点上实际执行过的操作，并在本数据库中执行。

<font color="red">主服务器的日志被从服务器的`I/O线程`读取，写到从服务器的日志中，再被从服务器的`SQL 线程`操作，从而写到服务器中</font>

# 9. 存储过程

优点:封装性高,不易进行SQL注入,且执行效率较高

缺点:可移植性比较差

> 使用方法

```
create procedure 存储过程名()
BEGIN
xxxxxxx
END;

cell 存储过程名();
```

可以携带参数

# 10. 三大范式

## 第一范式

<font color="red">就是具备原子性，不可分解。</font>

```sql
user表
 id  name   addr
100   ymy  上海市浦东新区 
```

1. 前面两个字段，都遵循第一范式
2. 第三个字段还可以继续分，也就是还可以分成"上海市"和"浦东新区"

## 第二范式

<font color="red">一定要有主键</font>

小问题：电商中的订单号，可以用主键id作为订单号吗？不行的，为什么，因为<font color="red">主键不能做具体的义务</font>。但是订单号可以作为唯一约束（所以也可以作为主键，不使用主键自增长）

企业中生成订单号，不可能使用uuid的，可能会产生幂等。

可以使用序列，分布式锁，redis等生成。主流是：提前将订单号生成号，让别人用一个取一个。快取没了的时候再生成。

订单号：字母+时间戳+用户主键+加密

## 第三范式

<font color="red">不允许字段有重复冗余</font>

```
user表

id name classid className
1 ymy	7218	十八班
```

这样，这个班级id和班级名就是冗余的

> 一般来说不一定百分百遵循第三范式
>
> 因为有时候也会有业务的要求

# Last

## 问题

### 「1」MyISAM和INNODB区别

InnoDb经过屡次优化,其实已经完全可以站在myIsam前面了

innoDb的优点

+ 支持事务
+ 支持表级锁,行级锁
+ 支持外键
+ 不会产生空间碎片

myIsam的优点

+ 支持全文索引

### 「2」锁类型

mysql锁分为共享锁(读锁)和排他锁(写锁)

共享锁,指锁情况下,允许进行其他的读操作.不允许写操作

排他锁,会阻塞其他的写锁和读锁

+ 表锁:会锁定整张表数据,在alter的时候会发生这个情况

+ 行锁

  + 悲观锁,顾名思义很悲观,每次进行数据操作的时候,都会加锁

    ```
    假设丢失更新一定存在；sql后面加上for update；这是数据库的一种机制。
    ```

  + 乐观锁

    认为操作的数据不会同时被别人操作,所以不会加锁,只是会执行后检查是否有存在修改的版本信息

### 「3」Undo log 和redo log

与事务有关的

undo log(记录的是原始数据) 、就是执行事务操作之前会将原始数据备份到log日志中.然后再进行数据修改,如果修改失败,这个时候就可以使用undo log进行数据恢复.**记录的是修改数据的原始数据**

redo log(记录的是新更新后的数据)、记录的是该事务更新的数据,也就是说需要更新的数据.在事务提交之前只需要持久化redo log即可,不需要持久化数据.

### 「4」分库分表

**什么是分区、分表、分库**

**分区**(使用partition关键字,多食用range的方式分区)

就是把一张表的数据分成N个区块，在逻辑上看最终只是一张表，但底层是由N个物理区块组成的，对业务透明，分区只不过把存放数据的文件分成了许多小块，根据一定的规则把数据文件(MYD)和索引文件（MYI）进行了分割，分区后的表呢，还是一张表。

**分表**

当数据量大到一定程度的时候，都会导致处理性能的不足，这个时候就没有办法了，只能进行分表处理。也就是把数据库当中数据根据按照分库原则分到多个数据表当中，这样，就可以把大表变成多个小表，不同的分表中数据不重复，从而提高处理效率。

**分库**

分表和分区都是基于同一个数据库里的数据分离技巧，对数据库性能有一定提升，但是随着业务数据量的增加，原来所有的数据都是在一个数据库上的，网络IO及文件IO都集中在一个数据库上的，因此CPU、内存、文件IO、网络IO都可能会成为系统瓶颈。

**雪花算法**

基于分库分表之后,常用的自增id的方式并不适用了,所以这个时候的id就需要使用一些特殊的规则来生成,比如雪花算法

实现

```
去配置信息，核心代码就是毫秒级时间41位 机器ID 10位 毫秒内序列12位。

第一位为未使用（实际上也可作为long的符号位），接下来的41位为毫秒级时间，然后5位datacenter标识位，5位机器ID（并不算标识符，实际是为线程标识），然后12位该毫秒内的当前毫秒内的计数，加起来刚好64位，为一个Long型。
```

### 「5」explain关键字

`explain select * from student`

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/NnyVeR.jpg)

**id**

id相同,标识多个查询条件,从上至下依次执行

id不同,标识其中嵌套有**子查询**.id越大,越先执行	

**select_type**

表示查询的方式

SIMPLE：简单的select查询，查询中不包含subquery或者union；

PRIMARY：查询中若包含复杂的子查询，最外层查询则被标记为PRIMARY；

SUBQUERY：在select中或者where列表中包含的子查询；

DERIVED：在from列表中包含的子查询被标记为DERIVED（衍生）MySQL会递归执行这些子查询，把结果放在临时表；

UNION：若第二个select出现在UNION后，则被标记为UNION；若UNION包含在FROM子句的子查询中，外层SELECT将被标记为DERIVED；

UNION RESULT：从UNION临时表检索结果的select；

select_type的作用：主要用于区分是否是子查询、联合查询、子查询等的复杂查询；

**table**

标识该条数据匹配的表

**type**

以上值从最好到最坏依次是：**system>const>eq_ref>ref>range>index>all**

1. system：表中只有一行记录（等于系统表），这就是const类型的特例，平时不会出现；
2. const：表示通过索引一次就可以找到，const用于比较primary key或者unique索引，因为只匹配一行数据，所以很快。如果主键置于where列表中，MySQL就能将查询转换成一个常量；
3. eq_ref：唯一性索引扫描，对于每个索引值，表中只有一条记录与之匹配，常用于主键或唯一索引扫描。
4. ref：非唯一性扫描，返回匹配某个结果单独值的所有行。本质上也是一种索引访问，它返回所有匹配某个单独值的行，然而，它可能会找到多个符合条件的行，所以它应该属于查找和扫描的混合体；
5. range：只检索指定范围的行，使用一个索引来选择行。key列显示使用了哪个索引，一般就是在你的where语句中出现了between、<、>、in等查询，这种范围扫描索引比全表扫描要好，因为只需开始索引的某一点，而结束另一点，不用扫描全部索引。
6. index：Full Index Scan，index与all的区别就是index类型只遍历索引树，这通常比all要快，因为索引文件通常比数据文件要小（也就是说虽然all和index都是全表扫描，但index是从索引中读取，而all是从磁盘中读取）。
7. all:全局检查,没有匹配到任何索引信息

**possible_keys**

查询到的数据所在列的所有索引信息

比如*,会列出所有这张表的索引

**key**

当前查询语句使用到的索引

**key_len**

该条查询语句为**索引字段最大可能长度**

**ref**

显示索引那一列是否被使用,如果被使用,则为常数(const)

**rows**

大致估算出查询到数据所需要浏览的行数

Extra(前三者是SQL里最重要的指标）

**（1）Using filesort：说明MySQL会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取，MySQL中无法利用索引完成的排序操作称为"文件排序"；**

**（2）Using temporary：使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表，常出现在order by排序和group by分组查询中；**

**（3）Using index：表示相应的select操作中使用了覆盖索引（Covering index），避免了访问了表的数据行，效率不错。如果同时出现了Using where，表明索引被用来执行索引键值的查找；如果没有同时出现Using where，表明索引用来读取数据而非执行查找动作；**

（4）Using where：表示查询使用了where条件；

（5）Using join buffer：表示查询使用了连接缓存；

（6）impossible where：where子句的值总是false，不能用来获取任何元祖；

（7）select tables optimized away：在没有group by 子句的情况下，基于索引优化min、max操作或者对于MyISAM存储引擎优化count(*)操作，不必到执行阶段在进行计算，查询执行计划生成的阶段即完成优化。

（8）ditinct：优化distinct操作，在找到第一匹配的元素即停止找同样的动作；

### 「6」count(*)、count(1)、count(column)的区别

***count(*)和count(1)**:返回获取的所有数据的总行数,包括数据为null的情况

**count(column)**:不会统计column字段为null的数据

### 「7」**是如何设计数据库的**

1. 首先设计数据库需要遵循三范式，即使不满足第三范式（由于业务逻辑的问题和查询效率问题），也一定要满足前两个范式
2. 最好使用UML建模来设计数据库的表。
3. 结构建立好了，还需要考虑性能问题，所以可以适当地建立索引

### 「8」为什么设计mysql要优化存储的数据结构

+ 空间上占用更小的空间
+ 在极限条件下,每行数据更小表明mysql的每页数据能存储更多的行数据,在一定条件下可以增加查询效率
+ 人为条件下也可以使用不同的数据类型来表示不同类型的存储情况,从而规范协作开发中的开发规范性问题

### 「9」为什么变长的varchar要尽可能小的符合业务需要

+ 在进行排序(需要临时空间)和创建临时表时申请临时空间,会使用指定的n来申请内存

### 「10」为什么字段尽量不要默认为null

1. 该列数据只要有一个是null,就会导致该列复合索引失效
2. null值干扰排序、分组、去重等结果
3. null值在进行复合函数的计算时会有问题
4. null不等同于空,在实际业务开发中会多出来一种逻辑

### 「11」一条sql语句是如何执行的

+ 参考极客时间《MySQL 实战45讲》的第一讲

### 「12」为什么要分库分表（高并发场景数据库如何设计）

单表的容纳上限有限制，不能太多。最大也就是到几百万，数据太多，`sql`性能太差

**并发量太高**，**数据量太高，查询速度太慢**

![](https://coderymy-image.oss-cn-beijing.aliyuncs.com/uPic/分库分表原理.png)

### 「13」用过哪些分库分表的中间件以及各自的特点

数据库中间件是获取数据，然后自己处理是将数据放在哪个表里面

常见的数据库中间件：cobar，TDDL，atlas，sharding-jdbc，mycat

proxy层：数据库独立部署在一台机器上的，需要相互通信

1. cobar，早些年使用，现在不使用了，不支持读写分离等
2. atlas，不维护了，不怎么使用了
3. <font color="red">mycat</font>：年轻，好用

client层：只是jar，所以也就是和系统是在一起的

1. TDDL，是淘宝团队开发的，不支持join，多表查询等语法。支持读写分离等。还依赖淘宝的diamond配置管理系统
2. <font color="red">sharding-jdbc</font>：很多大公司的使用，支持TCC事务

**比较sharding-jdbc和mycat**

前者由于是client层，所以不需要独立的部署或者维护。成本低。但是如果需要升级就比较麻烦。也就是耦合性很高

后者，有运维成本，但是proxy层所以和代码的耦合性不是很高。便于后期维护和重构

中小型建议使用sharding-jdbc。大型建议使用mycat（需要人手维护）

### 「14」如何对数据库进行垂直拆分和水平拆分（如何实现分库分表）



**「14.1 垂直拆分」**

拆分列（字段），将访问频率很高的字段拆出来。因为便于数据库缓存的使用

也就是将表拆开，订单表，订单详情表等

**「14.2 <font color="red">水平拆分</font>」**

一般我们说分库分表都是指水平拆分

1. 也就是将数据拆分开，将原本一个表中放的500万条数据，分给两个数据库中存放同样的表，但是每个表中只存储250万条数据
2. 将一个表拆成表结构一样**名字不一样**的的多个表，每个表中存储较少的数据



拆分方法可以是使用hash方法

>orderId%数据库的数量---->结果是多少就到对应的数据库中去
>
>比如有三个数据库。每个数据库中有四个表
>
>现在orderId是54
>
>分表的时候再进行一次
>
>54%3=0------到0号数据库中
>
>54%4==2-------到0号数据库的2号表中去
>
>优点是：可以分摊每个库的访问压力
>
>缺点是：扩容比较麻烦

拆分方法可以使用range方法

> 也就是按照时间来分。一段时间的数据放在一定的位置
>
> 好处是后期扩容的时候简单，只要预备好，给每个月都准备一个库就可以了。实际生产中使用range会看场景。有时候用户访问的并不是最新的数据，而是均匀访问时间的数据。这个时候就使用range的方法比较好





